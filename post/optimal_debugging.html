<!doctype html>
<html lang="en"><meta charset=utf-8>
<title>Towards an optimal debugging framework library.</title>
<style>
    #contents {
      max-width: 60em;
      margin: auto;
      padding: 0 1em;
    }
    code {
      background: #f8f8f8;
      border: 1px dotted silver;
      padding-left: 0.3em;
      padding-right: 0.3em;
    }
    pre > code {
      display: block;
      overflow: auto;
      padding: 0.5em;
      border: 1px solid #eee;
      line-height: normal;
    }
    .tok-kw {
        color: #333;
        font-weight: bold;
    }
    .tok-str {
        color: #d14;
    }
    .tok-builtin {
        color: #005C7A;
    }
    .tok-comment {
        color: #545454;
        font-style: italic;
    }
    .tok-fn {
        color: #900;
        font-weight: bold;
    }
    .tok-null {
        color: #005C5C;
    }
    .tok-number {
        color: #005C5C;
    }
    .tok-type {
        color: #458;
        font-weight: bold;
    }
    <!-- figure -->
    figure {
      margin: auto 0;
    }
    figure pre {
      margin-top: 0;
    }

    figcaption {
      padding-left: 0.5em;
      font-size: small;
      border-top-left-radius: 5px;
      border-top-right-radius: 5px;
    }
    figcaption.c-cap {
      background: #fcdba5;
    }
    figcaption.peg-cap {
      background: #fcdba5;
    }
    figcaption.javascript-cap {
      background: #365d95;
      color: #fff;
    }
    figcaption.shell-cap {
      background: #ccc;
      color: #000;
    }

</style>
<div id="contents">
<div style="text-align:left; padding up down: 1em">
<h2>Towards an optimal debugging framework library.</h2>
</div>
  This text is intended as overview of debugging techniques and motivation for
  uniform execution representation and setup to efficiently mix and match the
  appropriate technique for system level debugging with focus on statically
  optimizing compiler languages to keep complexity and scope limited.
  The author accepts the irony of such statements by "C having no ABI"/many
  systems in practice having no ABI, but reality is in this text simplified for
  brevity and sanity.
  <ol>
    <li><a href="#theory">Theory of debugging.</a></li>
    <li><a href="#practice">Practical methods with tradeoffs.</a></li>
    <li><a href="#uniform_execution_representation">Uniform execution representation.</a></li>
    <li><a href="#abstraction_problems">Abstraction problems during problem isolation.</a></li>
    <li><a href="#possible_implementations">Possible implementations.</a></li>
  </ol>
  <ol>
    <li><a href="#theory">Theory of debugging.</a></li>
      A program <a href="https://gu.outerproduct.net/debug.html">
      can be represented as (often non-deterministic) state machine</a>,
      such that a <b>bug</b> is a <b>bad transition rule</b> between those states.
      It is usually assumed that the developer/user knows correct and incorrect
      (bad) system states and the code represents a somewhat correct model of
      the intended semantics.
      Then an execution witness are the states and state transitions encountered
      on a specific program run. If the execution witness shows a "bad state",
      then there must be a bug.
      Thus a <b>debugger</b> can be seen <b>as query engine over states and transitions of
        a buggy execution witness.</b><br>
      Frequent operations are bug source isolation to deterministic components,
      where encapsulation of non-determinism usually simplifies the process.
      In contrast to that, concurrent code is tricky to debug, because one
      needs to trace multiple execution flows to estimate where the origin of the
      incorrect state is.<br>
      One can generally categorize methods into the following list (<b>asoul</b>)
      <ol>
        <li><b>automate</b> the process to minimize errors/oversights during debugging,
        against probabilistic errors, document the process etc
        </li>
        <li><b>simplify and isolate</b> system components and changes over time</li>
        <li><b>observe</b> the system while running it to <b>trace state or state changes</b></li>
        <li><b>understand</b> the <b>expected and actual code semantics</b> to the degree
        <li><b>learn, extend and ensure</b> how and which system invariants are satisfied</li>
          necessary from <b>of the involved systems</b> (for example userspace
          processes, kernel, build system, compiler, source code, linker,
          object code, assembly, hardware etc)
        </li>
      </ol>
      with the fundamental constrains being (<b>feel</b>)
      <ol>
        <li><b>finding out</b> correct system components semantics</li>
        <li><b>[e]ensuring</b> deterministic reproducibility of the problem</li>
        <li><b>limited</b> time and effort</li>
      </ol>

      Common debugging methods to <b>feel a soul</b> with various tradeoffs
      from compile-time to runtime debugging less to more run-time data
      collection:
      <ol>
        <li> <b>Formal Verification</b> as ahead or compile-time invariant resolving.</li>
        <li> <b>Validation</b> as runtime invariant checks.</li>
        <li> <b>Testing</b> as sample based runtime invariant checks.</li>
        <li> <b>Stepping</b> via "classical debugger" to manipulate task execution
          context, manipulate memory optionally via source code location translation
          via REPL commands, graphically, scripting or (rarely) freely programmable.</li>
        <li> <b>Logging</b> as dumping (a simplification of) state with context
          from bugs (usually timestamps in production systems).</li>
        <li> <b>Tracing</b> as dumping (a simplification of) runtime behavior
          via temporal relations (usually timestamps).</li>
        <li> <b>Recording</b> Encoded dumping of runtime to replay runtime with
          before specified time and state determinism.</li>
      </ol>

      Simplification and isolation means to apply the meaning of both words on
      all potential sub-components including, but not limited to
      hardware, code versioning including dependencies, source system,
      compiler framework and target system. Typical methods are
      <ol>
        <li> <b>Bisection</b> via git or the actual binaries.</li>
        <li> <b>Reduction</b> via removal of system parts or trying to reproduce
          with (a minimal) example.</li>
        <li> <b>Statistical analysis</b> from collected data on how the problem
          manifests on given environment(s) etc.</li>
      </ol>

      <b>Debugging</b> is domain- and design-specific and <b>relies on</b> core component(s)
      of <b>the to be debugged system to provide necessary debug functionality</b>.
      For example, software based hardware debugging relies on interfaces to
      the hardware like JTAG, Kernel debugging on Kernel compilation or
      configuration and elevated (user), userspace debugging on process and
      user permissions, system configuration or a child process to be debugged
      on Posix systems via ptrace.

    <li><a href="#practice">Practical methods with tradeoffs.</a></li>
    Usually semantics are not "set into stone" inclusive or do not offer
    sufficient tradeoffs, so formal verification is rarely an option aside of
    usage of models as design and planning tool.
    Depending on the domain and environment, problematic behavior of hardware
    or software components must be to be more or less 1. avoided and 2. traceable
    and there exist various (domain) metrics as decision helper.
    Very well designed systems explain users how to debug bugs regarding to
    <b>functional behavior</b>, <b>time behavior</b> with <b>internal and
      external system resources</b> themself up to the degree the system usage and
    task execution correctness is intended.
    Access restrictions limit or rule out stepping, whereas storage limitations
    limit or rule out logging, tracing and recording.


    TODO: requirements on system design for formal verification vs debugging.
    no surprise rule: core system enabling debugging (in any form) must be correct
    to the degree necessary.

    TODO: good argumentation on ignoring linker speak, language footguns etc.

      <ol>
        <li>Bugs related to functional behavior.</li>
        <li>Bugs related to time behavior.</li>
        <li>Internal and external system resources.</li>
      </ol>
    <!-- chapter 1 functional correct behavior -->
    <!-- memory problems -> logic problems -> language problems (detectable,
         undetectable undefined behavior, miscompilations) -->
    <!-- chapter 2 time correct behavior -->
    <!-- freezes -> performance problems (latency, throughput) -->
    <!-- chapter 3 abstracting problems away from hardware -->
    <!-- hw -> kernel -> user space resources -->
      <ol>
        <li>Debugging hard(ware) problems</li>
        <a href="https://interrupt.memfault.com/blog/schematic-review-checklist">
          Hardware design reviews</a> with extensive focus on core components
        (power, battery, periphery, busses, memory/flash and debug/test infrastructure)
        to enable debugging and component tests against product and assembling defects.
        TODO Elimination or mitigation of time channels, formal methods? attack fuzzing?
        software toggles?
        <li>Kernel and platform problems.</li>
        The managing environment the code is running on can vary a lot.
        As example, the typical four phases of the Linux boot process
        (system startup, bootloader stage, kernel stage, and init process)
        have each their own debugging infrastructure and methods.
        Generally, working with (introspection-restricted) platforms requires
        1. reverse engineering and "trying to find info" and/or 2. "use some tracing
        tool" and for 3. open source "adjust the source and stare at kernel
        dumps/use debugger".
        Kernels are rarely designed for tracing, recording or formal
        verification due to internal complexity and virtualization is slow and
        hides many classes of synchronization bugs.
        <li>Detectable Undefined Behavior</li>
        Compiler and runtime sanitizers
          <ol>
            <li>C</li>
            <li>C++</li>
            <li>Zig with <b>-OReleaseSafe</b> turns "undefined behavior" into
              runtime-checked disallowed behavior except for
              <ol>
                <li>TODO</li>
                <li>TODO</li>
                <li>TODO</li>
                <li>TODO</li>
              </ol>
            </li>
          </ol>
        <li>Undetectable Undefined Behavior</li>
        Staring at source code, backend intermediate representation like LLVM
        IR and reducing the problem or resulting assembly. Unfortunately the
        backend optimizers like LLVM do not offer frontend language writers
        debug APIs and related tooling due to not being designed for that
        purpose.
        <li>Miscompilations</li>
        Tools like Miri or Cerberus run the program in an interpreter,
        but may not cover all possible program semantics due to ambiguity
        and may not be feasible, so the only good chance is to reduce it.
        <li>Memory problems
          sanitizers, validators, simulator, tracers: TODO which, configurability and costs
          <ol>
            <li>out-of-bounds access</li>
            sanitizer
            <li>null pointer dereference</li>
            sanitizer
            <li>type confusion</li>
            sanitizer
            <li>integer overflow</li>
            sanitizer
            <li>use after free</li>
            sanitizer
            <li>invalid stack access</li>
            sanitizer
            <li>usage of uninitialized memory</li>
            sanitizer
            <li>data races</li>
            sanitizer
          </ol>
        </li>
        <li>Resource leaks (Freestanding/Kernel)</li>
            sanitizer
        <li>Freezes (deadlocks, softlocks, signal safety, unbounded loops etc)</li>
            sanitizer, validator
        <li>Performance problems</li>
            simulator, tracer
        <li>Logic problems of software systems can be described as problems
          related to incorrectly applied logic of how the code is solving the
          intended and follow-up problems ignoring hardware problems, kernel
          problems, different types of UB, miscompilations memory problems,
          resource leaks, freezes and performance issues.
          <ol>
            <li>(temporary) inconstency of state (relations)</li>
            <li>incorrect math ie for edge cases</li>
            <li>incorrect modeling of external and internal state and synchronization</li>
            <li>incorrect protocol handling</li>
            <li>insufficient handling of or the software requirements themself</li>
          </ol>
          The source of these problems are usually
          <ol>
            <li> incorrect constrains on the design, meaning how the different
              parts should interact and work towards the goals for the use
              cases</li>
            <li> unclear, unspecified or incorrectly assumed hardware or software
              guarantees by components </li>
            <li> implementation oversights, unintended use cases, unfeasibility
              of a general solution due to time and/or money constrains</li>
          </ol>
          Formal modeling of the design, model checking, code review, writing
          tests for edge cases or runtime validation are typically used with
          best practice being to write code in a risk-aware, testable and
          debuggable way. The methods and scope are here very wide and very
          domain and use case specific, so no general or short recommendation
          can be made.
        </li>
        <!-- https://www.cprogramming.com/tutorial/memory_debugging_parallel_inspector.html -->
      </ol>

      TODOs:
      <ol>
        <li>Tooling and performance tradeoffs.</li>
        <li>minimal descriptions for C, Rust, Zig; Posix, Linux, Windows</li>
      </ol>

      Ideally, only the system behavior and interactions with domain and
      use-case specific parts need cognitive load from the programmer, whereas
      the other error classes have standard approaches to isolate and eliminate.
      Unifying debug tooling simplifies usage for bigger developer productivity
      and exposing as library allows to automate this process.

      <!-- <li>Verification</li> -->
    <li><a href="#uniform_execution_representation">Uniform execution representation.</a></li>
      As it was shown before, modern languages simplify detection or elimination of
      memory problems and runtime detectable undefined behavior.
      So far undetectable undefined behavior may be detected, if backend optimizers
      are redesignede with according APIs.
      Detecting miscompilations requires strict formal reasoning of executing
      the source code semantics or formal verification of the compiler
      itself, which shall not be discussed here.
      This leaves hardware problems, kernel problems, resource leaks, freezes,
      performance problems and logic problems.
      TODO: what they have in common + motivation

      TODO: Uniform execution representation and queries over program execution.
    <li><a href="#abstraction_problems">Abstraction problems during problem isolation.</a></li>
      TODO: origin detection, isolation and abstraction
    <li><a href="#possible_implementations">Possible implementations.</a></li>
      TODO: (query system data vs modify the system vs other) to validate approaches;
      Program modification and validation language, query language and alternatives.
  </ol>
  <!-- Section 1: Theory, methods and determinism as fundamental problem.<br> -->
  <!-- Section 2: Practical methods with tradeoffs.<br> -->
  <!-- Section 3: Uniform execution representation and queries over program execution.<br> -->
  <!-- Section 4: Abstraction problems during problem isolation.<br> -->
  <!-- Section 5: Program modification and validation language, query language and alternatives.<br> -->
</div>
</html>
