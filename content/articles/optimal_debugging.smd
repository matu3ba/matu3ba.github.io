---
.title = "Towards optimal an optimal debugging library framework",
.author = "Jan Philipp Hafer",
.date = @date("2024-06-28:00:00:00"),
.layout = "optimal_debugging.shtml",
.tags = ["article", "system", "hardware", "software", "design", "debugging", "library"],
.draft = false,
---

[]($section.id("intro"))
This article is intended as overview of software based debugging techniques and motivation for
uniform execution representation and setup to efficiently mix and match the
appropriate technique for system level debugging with focus on statically
optimizing compiler languages to keep complexity and scope limited.
The reader may notice that there are several documented deficits
across platforms and tooling on documentation or functionality, which will be improved.
The author accepts the irony of such statements by "C having no ABI"/many systems in
practice having no stable or formally specified ABI, but reality is in this text simplified
for brevity and sanity.

Section 1 (theory) feels complete aside of simulation and hard-/software replacement
techniques and are good first drafts for bug, debugging and debugging process.
Section 2 (practical) is tailored towards non micro Kernels, which are based
on process abstraction, but is currently missing content and scalability numbers
for tooling.
The idea is to provide understanding and numbers to estimate for system design,
1 if formal proof of correctness is feasible and on what parts,
2 problems and methods applicable for dynamic system analysis.
Section 3 (future) will be on speculative and more advanced ideas, which should
be feasible based on numbers. They are planned to be about how to design
systems for rewriting and debugging using formal methods, compilers and
code synthesis.

- 1.[Theory of debugging](#theory)
- 2.[Practical methods with trade-offs](#practice)
- 3.[Uniform execution representation](#uniform_execution_representation)
- 4.[Abstraction problems during problem isolation](#abstraction_problems)
- 5.[Possible implementations](#possible_implementations)

[]($section.id("theory"))
### Theory of debugging

A (software) system [can be represented as (often non-deterministic) state machine](https://gu.outerproduct.net/debug.html),
such that a **bug** is a **bad transition rule** between those states.
It is usually assumed that the developer/user knows correct and incorrect
(bad) system states and the code represents a somewhat correct model of
the intended semantics.
Then an execution witness are the states and state transitions encountered
on a specific program run. If the execution witness shows a "bad state",
then there must be a bug.
Thus a **debugger** can be seen **as query engine over states and transitions of
a buggy execution witness.**  
In more simple terms, **debugging is not making bugs or removing them**.  
Frequent operations are bug source isolation to deterministic components,
where encapsulation of non-determinism usually simplifies the process.
In contrast to that, concurrent code is tricky to debug, because one
needs to trace multiple execution flows to estimate where the origin of the
incorrect state is.

The process of debugging means to use static and dynamic (software) system analysis
and its automation and adaption to speed up bug (classes) elimination for the
(classes of) target systems.

One can generally categorize methods into the following list [**a**utomate, **s**implify, **o**bserve, **u**nderstand, **l**earn] (**asoul**)
- **a**utomate the process to minimize errors/oversights during debugging,
  against probabilistic errors, document the process etc
- **s**implify and isolate system components and changes over time
- **o**bserve the system while running it to *trace state or state changes*
- **u**nderstand the expected and actual code semantics to the degree necessary
- **l**earn, extend and ensure how and which system invariants are satisfied
  necessary from *of the involved systems*,
  for example user-space processes, kernel, build system, compiler, source code, linker,
  object code, assembly, hardware etc

with the fundamental constrains being [**f**inding, **ee**nsuring, **l**imited] (**feel**)
- **f**inding out correct system components semantics
- **ee**nsuring deterministic reproducibility of the problem
- **l**imited time and effort

Common static and dynamic (software) system analysis methods to
**run the system** to **feel a soul** for the purpose of eliminating the bug
(classes) are:
- **Specification** meaning to "compare/get/write the details", possibly formally, possibly for (software) system synthesis.
- **Formal Verification** as ahead or compile-time invariant resolving.
  May be superflous by (software) system synthesis based on **Specification** or unfeasible
  due to complexity or non-formal specification.
- **Validation** as runtime invariant checks. Sanitizers as compiler runtime checks are common tools.
- **Testing** as sample based runtime invariant checks. Coverage based fuzzers are common tools.
- **Stepping** via "classical debugger" to manipulate task execution
  context, manipulate memory optionally via source code location translation
  via REPL commands, graphically, scripting or (rarely) freely programmable.
- **Logging** as dumping (a simplification of) state with context
  from bugs (usually timestamps in production systems).
- **Tracing** as dumping (a simplification of) runtime behavior
  via temporal relations (usually timestamps). Can be immediate or sampled.
- **Recording** Encoded dumping of runtime to replay runtime with
  before specified time and state determinism.
- **Scheduling** meaning to do logical or time-relation based scheduling of
  process or threads. Typical use cases are undo "thread fuzzing", rr "chaos
  mode", using the kernel scheduler API or bounded model checking.
- **Reversal computing** meaning to reverse execute some code to (partial) reset
  the system to a previous state without **Recording** and replaying.
  Typically used in simulations and pure logic functionality of languages and
  corresponds to applying some bijective function.
- **Time-reversal computing** to do **Reversal computing** with tracked time.
  Mostly used in simulations, because (if used) source code to assembly
  relation and (assembly) instruction time must be fixed and known.

The core ideas for **what software system to run** based on code with its
semantics are then typically a mix of
- **Machine code** execution on the actual hardware to get hardware and timing behavior.
- **Simulation** as **partial or full execution** on a simplified, imitative
  representation of the target hardware to get information for the simplified model.
- **Virtualisation** as **isolation or simplification** of a hardware- or software
  subsystem to reduce system complexity.

Further, isolation and simplification are typically applied on all potential
sub-components including, but not limited to hardware, code versioning
including dependencies, source system, compiler framework and target system.
Methods are usually
- **Bisection** via git or the actual binaries.
- **Reduction** via removal of system parts or trying to reproduce with
  (a minimal) example.
- **Statistical analysis** from collected data on how the problem
  manifests on given environment(s) etc.

**Debugging** is domain- and design-specific and **relies on** core component(s)
of **the to be debugged system to provide necessary debug functionality**.
For example, software based hardware debugging relies on interfaces to
the hardware like JTAG, kernel debugging on kernel compilation or
configuration and elevated (user), user-space debugging on process and
user permissions, system configuration or a child process to be debugged
on Posix systems via `ptrace`.

Without costly hardware devices to trace and physical access to the computing unit
for exact recording of the system behavior including time information,
dynamic (software) system analysis (to run the system) requires trade-offs on what
program parts and aspects to inspect and collect data from.
Therefore, it depends on many factors, for example bug classes and target
systems, to what degree the process of debugging can and should be automated or
optimized.

[]($section.id("practice"))
### Practical methods with trade-offs

Depending on the domain and environment, problematic behavior of hardware
or software components must be (more or less) 1 avoided or 2 traceable
and there exist various (domain) metrics as decision helper.
Very well designed systems explain users how to debug regarding to
**functional behavior**, **time behavior** with **internal and
external system resources** up to the degree the system usage and
task execution correctness is intended.
Access restrictions limit or rule out stepping, whereas storage limitations
limit or rule out logging, tracing and recording.

Formal methods, **Specification**, (software) system synthesis and **Formal Verification**

(Highly) safety-critical systems or hardware are typically created from formal **Specification**
by (software) system synthesis or, when (full) synthesis is unfeasible, implementations are formally verified.
To my knowledge no standards for (highly) security-critical systems exist,
which require formal **Specification** and **Formal Verification** or synthesis (2025-05-16).

For non safety- or security-critical or hardware (sub)systems, usually
semantics are not "set into stone", so **Formal Verification** or (software) system
synthesis is rarely an option.
Formal models and (semi-)formal specifications are however commonly used for
design, planning, testing, review and validation of fail-safe or core (software) system
functionality.

Typical used models for C, C++, Zig and compiler backends are
Integer Arithmetic, Modular Arithmetic, Saturation Arithmetic for integers and
Floating point arithmetic (with possible rough edge cases like signaling NaN propagation),
Fixed-Point Arithmetic for real numbers.
(Simplified) instances of Separation Logic may be used to model and check
pointers and resources, for example Safe Rust uses separation logic with
lifetime inference and user annotations based on strict aliasing of Unsafe Rust.

Typical relevant unsolved or incomplete models for compilers are
1. hardware semantics, specifically around timing behavior and (if used) weak memory
2. memory synchronization semantics for weak memory systems with ideas from
"Relaxed Memory Concurrency Re-executed" and suggested model looking promising
3. SIMD with specifically floating point NaN propagation
4. pointer semantics, specifically in object code (initialization), se- and deserialization,
  construction, optimizations on pointers with arithmetic, tagging
5. constant time code semantics, for example how to ensure data stays in L1, L2 cache
  and operations have constant time
6. ABI semantics, since specifications are not formal

and typical problems more related to platforms like Kernels are
1. resource (tracking) semantics, for example how to track resources in a process group
2. security semantics, for example how to model process group permissions.

For **Validation**, **Sanitizers** are typically used as the most efficient and simplest
debugging tools for C and C++, whereas Zig implements them, besides thread
sanitizer, as allocator and safety mode.
Instrumented sanitizers have a 2x-4x slowdown vs dynamic ones with 20x-50x slowdown.

Nr | Clang usage                  | Zig usage         | Memory           | Runtime  | Comments                            |
-- | ---------------------------- | ----------------- | ---------------- | -------- | ----------------------------------- |
1  | -fsanitize=address           | alloc + safety    | 1x (3x stack)    | 2x       | Clang 16+ TB of virt mem            |
2  | -fsanitize=leak              | allocator         | 1x               | 1x       | on exit ?x? more mem+time           |
3  | -fsanitize=memory            | unimplemented     | 2-3x             | 3x       |                                     |
4  | -fsanitize=thread            | -fsanitize=thread | 5-10x+1MB/thread | 5-15x    | Clang ?x? ("lots of") virt mem      |
5  | -fsanitize=type              | unimplemented     | ?                | ?        | not enough data                     |
6  | -fsanitize=undefined         | safety mode       | 1x               | ~1x      |                                     |
7  | -fsanitize=dataflow          | unimplemented     | 1-2x?            | 1-4x?    | wip, get variable dependencies      |
8  | -fsanitize=memtag            | unimplemented     | ~1.0Yx?          | ~1.0Yx?  | wip, address cheri-like ptr tagging |
9  | -fsanitize=cfi               | unimplemented     | 1x               | ~1x      | forward edge ctrl flow protection   |
10 | -fsanitize=safe-stack        | unimplemented     | 1x               | ~1x      | backward edge ctrl flow protection  |
11 | -fsanitize=shadow-call-stack | unimplemented     | 1x               | ~1x      | backward edge ctrl flow protection  |

Sanitizers 1-6 are recommended for testing purpose and 7-11 for production by LLVM.
Memory and slowdown numbers are only reported for LLVM sanitizers. Zig does not
report own numbers yet (2025-01-11). Slowdown for dynamic sanitizer versions
increases by a factor of 10x in contrast to the listed static usage costs.
The leak sanitizer does only check for memory leaks, not other system resources.
Besides various kernel specific tools to track system resources,
Valgrind can be used on Posix systems for non-memory resources and
Application Verifier for Windows.
Address and thread sanitizers can not be combined in Clang and combined usage
of the Zig implementation is limited by virtual memory usage.
In Zig, aliasing can currently not be sanitized against, whereas in Clang only
typed based aliasing can be sanitized without any numbers reported by LLVM yet.

Besides adjusting source code semantics via 1 sanitizers, one can do 2 own dynamic
source code adjustments or use 3 tooling that use kernel APIs to trace and optionally
3.1 run-time check information or 3.2 run-time check kernel APIs and with underlying state.
Kernels further may simplify access to information, for example the `proc` file
system simplifies access to process information.

**Testing** is very context and use-case dependent with
typical separations being between pure/impure, time-invariant/variant,
accurate/approximate, hardware/software (sub)system separation from simple
unit tests up to integration and end to end tests based on
statistical/probability analysis and system intuition on determinstic expected
behavior based on explicit or implicit requirements.
TODO tools, hardware, software, mixed hw/sw examples

**Stepping**
* TODO time costs, sync options, etc

**Logging**
* TODO

**Tracing**
* TODO
  - [ ] "Debugging And Profiling .NET Core Apps on Linux"
  - [ ] https://github.com/goldshtn/linux-tracing-workshop
  - [ ] CPU sampling linux perf, bcc; win ETW; macos; macos instruments dtrace
  - [ ] dynamic tracing linux perf, systemtap, bcc; win nothing; macos dtrace
  - [ ] static tracing linux LTTng, win ETW, macos nothing
  - [ ] dump gen linux core_pattern, gcore; win procdump, WER; macos kern.corefile, gcore
  - [ ] dump analysis gdb,lldb; visual studio, windbg, gdb,lldb
  - [ ] lwn.net Unifying kernel tracing
  - [ ] https://github.com/goldshtn/linux-tracing-workshop
  - [ ] babeltrace https://babeltrace.org/
  - [ ] There are no "works for all kernels" and "trace specific (group of) processes" solutions,
  - [ ] so one has to do specific queries to constrain what data should be collected.
  - [ ] For low latency overhead analysis, dtrace or inspired systems like bpftrace,
  - [ ] bcc and systemtap can be used.
  - [ ] ETW allows complete user-space captures
  - [ ] Most related solutions use dtrace or
  - [ ] TODO
  - [ ] * list standard Kernel tracing tooling,
  - [ ] * focus on dtrace and drawback of no "works for all kernels" "trace processes"
  - [ ] * standard tooling for checking traced information
  - [ ] * Tracers: dtrace, bpftrace, bcc, systemtap, ETW, darwin/macos?, other posix tools?
  - [ ]   - TODO memory/runtime/latency overhead etc

**Recording**
* TODO requirements: eliminate non-deterministic choices for replaying, others

**Scheduling**
* TODO requirements: simplification methods, practicality

**Reversal computing**
* TODO how and when to write bijective code to simplify debugging

**Time-reversal computing**
* TODO use cases

The following is a list of typical problems with simple solution tactics.
To keep analysis simple, no virtual machine/emulator and simulation approaches are given.

[]($section.id("uniform_execution_representation"))
### Uniform execution representation

As it was shown before, modern languages simplify detection or elimination of
memory problems and runtime detectable undefined behavior. So far undetectable
undefined behavior may be automatically reduced, if backend optimizers are
redesigned with according reduction APIs.
Detecting miscompilations requires strict formal reasoning of executing the
source code semantics or formal verification of the compiler itself,
which shall not be discussed here.
This leaves hardware problems, kernel problems, resource leaks, freezes,
performance problems and logic problems.

1. leave hardware problems out for simplicity.
2. resource leaks are a special case of platform problems, because platform
provides resources.
Automatically tracking resource leaks requires Valgrind logic over all
memory operations, reduction requires (limited) kernel object tracing.
Tracing platform solutions will always have trade-offs.
Complete solution tracing user process and related kernel logic is only
available as dtrace with non-optimal performance.

TODO: (currently unused) what they have in common + motivation
TODO: Uniform execution representation and queries over program execution.

[]($section.id("abstraction_problems"))
### Abstraction problems during problem isolation

TODO: origin detection, isolation and abstraction

[]($section.id("possible_implementations"))
### Possible implementations

TODO: (currently unused)
query system data vs modify the system vs other to validate approaches;
Program modification and validation language, query language and alternatives.

